{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a7abb6-b1ff-473f-b34b-24f15fa50538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row with mismatched columns: expected 4, found 1\n",
      "Data scraped and saved to premier_league_overall_stats_EE.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_table(url, table_class):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', class_=table_class)\n",
    "    if table is None:\n",
    "        print(f\"Couldn't find the table with class {table_class}.\")\n",
    "        return None\n",
    "\n",
    "    # Extract headers from the first row of the table body\n",
    "    tbody = table.find('tbody')\n",
    "    if tbody is None or len(tbody.find_all('tr')) == 0:\n",
    "        print(\"No rows found in the table body.\")\n",
    "        return None\n",
    "\n",
    "    # Assuming the first row contains headers\n",
    "    headers = [cell.text.strip() for cell in tbody.find_all('tr')[0].find_all('td')]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Extract data from the remaining rows\n",
    "    for row in tbody.find_all('tr')[1:]:  # Skip the first row (header)\n",
    "        cells = row.find_all('td')\n",
    "        row_data = [cell.text.strip() for cell in cells]\n",
    "        \n",
    "        if len(row_data) == len(headers):\n",
    "            rows.append(row_data)\n",
    "        else:\n",
    "            print(f\"Skipping row with mismatched columns: expected {len(headers)}, found {len(row_data)}\")\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://footballdatabase.com/ranking/england/1'\n",
    "\n",
    "# Table class\n",
    "table_class = 'table table-hover'  # Modify this to match the table's class attribute\n",
    "\n",
    "# Scrape the table\n",
    "df = scrape_table(url, table_class)\n",
    "\n",
    "# Check if the DataFrame is not empty\n",
    "if df is not None and not df.empty:\n",
    "    # Save the DataFrame to a CSV file\n",
    "    file_path = 'premier_league_overall_stats_EE.csv'\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Data scraped and saved to {file_path}\")\n",
    "else:\n",
    "    print(\"Data could not be scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9dd1e1b-b2d5-4c3a-bdf1-28d6e4275af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://footballdatabase.com/ranking/europe/1\n",
      "Scraping https://footballdatabase.com/ranking/europe/2\n",
      "Scraping https://footballdatabase.com/ranking/europe/3\n",
      "Scraping https://footballdatabase.com/ranking/europe/4\n",
      "Scraping https://footballdatabase.com/ranking/europe/5\n",
      "All data scraped and saved to combined_european_league_stats_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_table(url, table_class):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', class_=table_class)\n",
    "    if table is None:\n",
    "        print(f\"Couldn't find the table with class {table_class}.\")\n",
    "        return None\n",
    "\n",
    "    # Dynamically extract headers\n",
    "    header_row = table.find('thead').find('tr') if table.find('thead') else table.find('tbody').find('tr')\n",
    "    headers = [cell.text.strip() for cell in header_row.find_all(['th', 'td'])]\n",
    "\n",
    "    rows = []\n",
    "    for row in table.find('tbody').find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        if not cells:\n",
    "            continue  # Skip rows without data cells\n",
    "        row_data = [cell.text.strip() for cell in cells]\n",
    "        rows.append(row_data)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scrape_multiple_pages(base_url, table_class, total_pages):\n",
    "    all_data = []\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        url = f\"{base_url}{page}\"\n",
    "        print(f\"Scraping {url}\")\n",
    "\n",
    "        df = scrape_table(url, table_class)\n",
    "        if df is not None and not df.empty:\n",
    "            all_data.append(df)\n",
    "        else:\n",
    "            print(f\"Data could not be scraped for page {page}.\")\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# Base URL and table class\n",
    "base_url = 'https://footballdatabase.com/ranking/europe/'\n",
    "table_class = 'table table-hover'\n",
    "\n",
    "# Scrape all 5 pages\n",
    "combined_df = scrape_multiple_pages(base_url, table_class, 5)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "if not combined_df.empty:\n",
    "    file_path = 'combined_european_league_stats_fixed.csv'\n",
    "    combined_df.to_csv(file_path, index=False)\n",
    "    print(f\"All data scraped and saved to {file_path}\")\n",
    "else:\n",
    "    print(\"No data was scraped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
